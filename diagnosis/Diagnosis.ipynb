{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use text classification to automatically classify diseases based on the user's symptoms and feelings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **User_input**: user inputs tokenized without preprocessing\n",
    "- **User_input_preprocessed**: user inputs tokenized with preprocessing\n",
    "- **User_input_preprocessed_stem**: user inputs tokenized with preprocessing and stemming\n",
    "- **User_input_preprocessed_lem** : user inputs tokenized with preprocessing and lematization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from hyperopt import hp, fmin, tpe, Trials, space_eval\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Conv1D, GlobalMaxPooling1D, LSTM, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run with GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration = tf.compat.v1.ConfigProto()\n",
    "#configuration.gpu_options.allow_growth = True\n",
    "#session = tf.compat.v1.Session(config=configuration)\n",
    "#tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "datapath = \"Data/dataset.csv\"\n",
    "\n",
    "# Read dataset into a dataframe\n",
    "data = pd.read_csv(datapath)\n",
    "\n",
    "# Check loading\n",
    "print(\"Data loaded successfully\") if data.any(axis=None) else print(\"Error loading data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset contains {data.shape[0]} rows and {data.shape[1]} columns.\" )\n",
    "print(f\"Dataset has {data['Disease'].nunique()} different diseases: {data['Disease'].unique()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset balancing\n",
    "data['Disease'].value_counts().plot(kind='bar', title='Dataset balancing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, X_Test, y_train, y_test):\n",
    "    # Train\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print(\"\\033[1mTraining Results\\033[0m\")\n",
    "    print(f\"Accuracy Score: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"Specificity Score: {recall_score(y_train, y_train_pred, average='weighted', labels=np.unique(y_test)):.4f}\")\n",
    "    print(f\"Precision Score: {precision_score(y_train, y_train_pred, average='weighted'):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_train, y_train_pred, average='weighted'):.4f}\")\n",
    "\n",
    "    # Test\n",
    "    y_test_pred = model.predict(X_Test)\n",
    "    print(\"\\n\\033[1mTesting Results\\033[0m\")\n",
    "    print(f\"Accuracy Score: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"Specificity Score: {recall_score(y_test, y_test_pred, average='weighted', labels=np.unique(y_test)):.4f}\")\n",
    "    print(f\"Precision Score: {precision_score(y_test, y_test_pred, average='weighted'):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_test_pred, average='weighted'):.4f}\")\n",
    "    \n",
    "    # Classification report train\n",
    "    print(\"\\n\\033[1mClassification Report Train\\033[0m\")\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "    # Classification report test\n",
    "    print(\"\\n\\033[1mClassification Report Test\\033[0m\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    # Figure for confusion matrix and roc\n",
    "    fig, (ax1, ax2)= plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = ConfusionMatrixDisplay(confusion_matrix(y_test, y_test_pred), display_labels=model.classes_)\n",
    "    conf_matrix.plot(ax=ax1)\n",
    "\n",
    "    # Roc Curve for each disease\n",
    "    prediction = model.predict_proba(X_Test)\n",
    "    falsepositive, truepositive, thresh = {}, {}, {}\n",
    "    for i, disease in enumerate(model.classes_):\n",
    "        falsepositive[disease], truepositive[disease], thresh[disease] = roc_curve(y_test, \n",
    "                                                                        prediction[:, i], \n",
    "                                                                        pos_label=disease)\n",
    "    \n",
    "    ax2.set_title('ROC Curve')\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    for disease in model.classes_:\n",
    "        ax2.plot(falsepositive[disease], truepositive[disease], label=disease)\n",
    "    \n",
    "    ax2.plot()\n",
    "\n",
    "    #return test f1score\n",
    "    return f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "\n",
    "def evaluate_dp(model, history, X, Y):\n",
    "    print(\"\\n\\033[1mTesting Results\\033[0m\")\n",
    "    loss, accuracy = model.evaluate(X, Y, verbose=-1)\n",
    "    print(\"Loss: \", loss)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "    figure_dp, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Val'])\n",
    "    ax1.plot()\n",
    "\n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Val'])\n",
    "    ax2.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['User_input_preprocessed'] = data['User_input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove stop words - Stop words are frequently occuring words in a language that are commonly omitted from NLP tasks due to their low significance for deciphering textual meaning.\n",
    "- Remove pontuations\n",
    "- Remove words with a single letter\n",
    "- Transfrom all text in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english stop words using stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "data['User_input_preprocessed'] = data['User_input_preprocessed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "# Remove words with a sigle letter\n",
    "data['User_input_preprocessed'] = data['User_input_preprocessed'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 1]))\n",
    "# Transform in lower case\n",
    "data['User_input_preprocessed'] = data['User_input_preprocessed'].str.lower()\n",
    "# Remove special chars and pontuation like '.' ','\n",
    "data['User_input_preprocessed'] = data['User_input_preprocessed'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "# Remove digits\n",
    "data['User_input_preprocessed'] = data['User_input_preprocessed'].str.replace('\\d+', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General word cloud\n",
    "word_list = \" \".join(text for text in data['User_input'])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, max_words = 100).generate(word_list)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Psoriasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psoriasis = data[data['Disease'] == 'Psoriasis']\n",
    "\n",
    "word_list = \" \".join(text for text in psoriasis['User_input_preprocessed'])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, max_words = 100).generate(word_list)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melanoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melanoma = data[data['Disease'] == 'Melanoma']\n",
    "\n",
    "word_list = \" \".join(text for text in melanoma['User_input_preprocessed'])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, max_words = 100).generate(word_list)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Urticaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urticaria = data[data['Disease'] == 'Urticaria']\n",
    "\n",
    "word_list = \" \".join(text for text in urticaria['User_input_preprocessed'])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, max_words = 100).generate(word_list)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lupus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lupus = data[data['Disease'] == 'Lupus']\n",
    "\n",
    "word_list = \" \".join(text for text in lupus['User_input_preprocessed'])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, max_words = 100).generate(word_list)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dermatitis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dermatitis = data[data['Disease'] == 'Dermatitis']\n",
    "\n",
    "word_list = \" \".join(text for text in dermatitis['User_input_preprocessed'])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, max_words = 100).generate(word_list)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization by white space\n",
    "#tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "#data['User_input_token_whitespace'] = data['User_input_preprocessed'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "# Tokenization by tree bank\n",
    "#tokenizer = nltk.tokenize.TreebankWordDetokenizer()\n",
    "#data['User_input_token_treebank'] = data['User_input_token_whitespace'].apply(lambda x: tokenizer.detokenize(x))\n",
    "\n",
    "# Tokenization by word punct\n",
    "#tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "#data['User_input_token_wordpunct'] = data['User_input_preprocessed'].apply(lambda x: tokenizer.tokenize(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming & Lamitization\n",
    "\n",
    "### Stemming\n",
    "Stemming is the process of reducing words to their word stems or roots by removing suffixes or prefixes. It uses simple rules to chop off ends of words, potentially resulting in non-real words.\n",
    "\n",
    "### Lemmatization\n",
    "Lemmatization, on the other hand, involves reducing words to their base or canonical form, known as the lemma, by considering the morphological analysis of the word. This process involves dictionary lookup to find the lemma, making it more accurate but potentially slower than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "data['User_input_preprocessed'] = data['User_input_preprocessed'].apply(lambda x: tokenizer.tokenize(x))\n",
    "data['User_input'] = data['User_input'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "data['User_input_preprocessed_stem'] = data['User_input_preprocessed'].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "data['User_input_preprocessed_lem'] = data['User_input_preprocessed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)\n",
    "Is a technique in NLP that represents text as a frequency count of words in a corpus, creating a numerical vector representing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "boW = CountVectorizer(stop_words='english')\n",
    "\n",
    "bow_normal = boW.fit(data['User_input'].apply(' '.join))\n",
    "\n",
    "bow_preprocessed = boW.fit(data[\"User_input_preprocessed\"].apply(' '.join))\n",
    "\n",
    "bow_lem = boW.fit(data['User_input_preprocessed_lem'].apply(' '.join))\n",
    "\n",
    "bow_stem = boW.fit(data['User_input_preprocessed_stem'].apply(' '.join))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think that to understand the symptoms is important to use n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_ngram = CountVectorizer(stop_words='english', ngram_range=(4,4))\n",
    "\n",
    "bow_ngram_normal = bow_ngram.fit(data['User_input'].apply(' '.join))\n",
    "\n",
    "bow_ngram_preprocessed = bow_ngram.fit(data[\"User_input_preprocessed\"].apply(' '.join))\n",
    "\n",
    "bow_ngram_lem = bow_ngram.fit(data['User_input_preprocessed_lem'].apply(' '.join))\n",
    "\n",
    "bow_ngram_stem = bow_ngram.fit(data['User_input_preprocessed_stem'].apply(' '.join))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "tf_idf_normal = tf_idf.fit(data['User_input'].apply(' '.join))\n",
    "\n",
    "tf_idf_preprocessed = tf_idf.fit(data['User_input_preprocessed'].apply(' '.join))\n",
    "\n",
    "tf_idf_lem = tf_idf.fit(data['User_input_preprocessed_lem'].apply(' '.join))\n",
    "\n",
    "tf_idf_stem = tf_idf.fit(data['User_input_preprocessed_stem'].apply(' '.join))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smoothing to avoid 0 probabilities\n",
    "#data_tfidf = data_tfidf + 1\n",
    "\n",
    "# Normalize\n",
    "#data_tfidf = data_tfidf.div(data_tfidf.sum(axis=1), axis=0)\n",
    "\n",
    "#display(data_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data spliting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset was created by the group, we decided to create a balanced dataset to make the next steps easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 8267 rows and 5 columns.\n",
      "Test dataset contains 2067 rows and 5 columns.\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets\n",
    "# Create a test and a train dataframe\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check splitting\n",
    "print(f\"Train dataset contains {train.shape[0]} rows and {train.shape[1]} columns.\")\n",
    "print(f\"Test dataset contains {test.shape[0]} rows and {test.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without preprocessing\n",
    "X_train_bow_noproc = bow_normal.transform(train['User_input'].apply(' '.join))\n",
    "X_test_bow_noproc = bow_normal.transform(test['User_input'].apply(' '.join))\n",
    "\n",
    "# With preprocessing\n",
    "X_train_bow_proc = bow_preprocessed.transform(train['User_input_preprocessed'].apply(' '.join))\n",
    "X_test_bow_proc = bow_preprocessed.transform(test['User_input_preprocessed'].apply(' '.join))\n",
    "\n",
    "# With stem\n",
    "X_train_bow_stem = bow_stem.transform(train['User_input_preprocessed_stem'].apply(' '.join))\n",
    "X_test_bow_stem = bow_stem.transform(test['User_input_preprocessed_stem'].apply(' '.join))\n",
    "\n",
    "# With lemmatization\n",
    "X_train_bow_lem = bow_lem.transform(train['User_input_preprocessed_lem'].apply(' '.join))\n",
    "X_test_bow_lem = bow_lem.transform(test['User_input_preprocessed_lem'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without preprocessing\n",
    "X_train_ngram_noproc = bow_ngram_normal.transform(train['User_input'].apply(' '.join))\n",
    "X_test_ngram_noproc = bow_ngram_normal.transform(test['User_input'].apply(' '.join))\n",
    "\n",
    "# With preprocessing\n",
    "X_train_ngram_proc = bow_ngram_preprocessed.transform(train['User_input_preprocessed'].apply(' '.join))\n",
    "X_test_ngram_proc = bow_ngram_preprocessed.transform(test['User_input_preprocessed'].apply(' '.join))\n",
    "\n",
    "# With stem\n",
    "X_train_ngram_stem = bow_ngram_stem.transform(train['User_input_preprocessed_stem'].apply(' '.join))\n",
    "X_test_ngram_stem = bow_ngram_stem.transform(test['User_input_preprocessed_stem'].apply(' '.join))\n",
    "\n",
    "# With lemmatization\n",
    "X_train_ngram_lem = bow_ngram_lem.transform(train['User_input_preprocessed_lem'].apply(' '.join))\n",
    "X_test_ngram_lem = bow_ngram_lem.transform(test['User_input_preprocessed_lem'].apply(' '.join))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without preprocessing\n",
    "X_train_tfidf_noproc = tf_idf_normal.transform(train['User_input'].apply(' '.join))\n",
    "X_test_tfidf_noproc = tf_idf_normal.transform(test['User_input'].apply(' '.join))\n",
    "\n",
    "# With preprocessing\n",
    "X_train_tfidf_proc = tf_idf_preprocessed.transform(train['User_input_preprocessed'].apply(' '.join))\n",
    "X_test_tfidf_proc = tf_idf_preprocessed.transform(test['User_input_preprocessed'].apply(' '.join))\n",
    "\n",
    "# With stem\n",
    "X_train_tfidf_stem = tf_idf_stem.transform(train['User_input_preprocessed_stem'].apply(' '.join))\n",
    "X_test_tfidf_stem = tf_idf_stem.transform(test['User_input_preprocessed_stem'].apply(' '.join))\n",
    "\n",
    "# With lemmatization\n",
    "X_train_tfidf_lem = tf_idf_lem.transform(train['User_input_preprocessed_lem'].apply(' '.join))\n",
    "X_test_tfidf_lem = tf_idf_lem.transform(test['User_input_preprocessed_lem'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Disease']\n",
    "y_test = test['Disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "**Important Parameters:**\n",
    "- Solver\n",
    "- Penalty\n",
    "- C\n",
    "- Max_Iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_lr = 0\n",
    "best_modeltype_lr = \"\"\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_lr(X_train):\n",
    "    space = {\n",
    "        'solver': hp.choice('solver', ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']),\n",
    "        'penalty': hp.choice('penalty', ['l2']),\n",
    "        'C': hp.uniform('C', 1.0, 10.0),\n",
    "        'max_iter': hp.choice('max_iter', [1, 2, 5, 10, 20, 50, 100, 200, 500])\n",
    "    }\n",
    "\n",
    "    def objective(params):\n",
    "        clf = LogisticRegression(solver=params['solver'], penalty=params['penalty'], C=params['C'], max_iter=params['max_iter'])\n",
    "        score = cross_val_score(clf, X_train.toarray(), y_train, cv=5).mean()\n",
    "        return -score\n",
    "\n",
    "    tpe_algorithm = tpe.suggest\n",
    "\n",
    "    trials = Trials()\n",
    "\n",
    "    best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe_algorithm,\n",
    "            max_evals=50,  \n",
    "            trials=trials)\n",
    "\n",
    "    best_params = space_eval(space, best)\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    return best_params\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_lr(X_train):\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    param_grid = [\n",
    "        {\n",
    "            'solver': ['liblinear'],\n",
    "            'penalty': ['l1','l2'],\n",
    "            'C': np.arange(1.0, 10.0, 1.0),\n",
    "            'max_iter': [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "        },\n",
    "        {\n",
    "            'solver': ['lbfgs','newton-cg','sag'],\n",
    "            'penalty': ['l2',],\n",
    "            'C': np.arange(1.0, 10.0, 1.0),\n",
    "            'max_iter': [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "        },\n",
    "        {\n",
    "            'solver': ['saga'],\n",
    "            'penalty': ['l1','l2'],\n",
    "            'C': np.arange(1.0, 10.0, 1.0),\n",
    "            'max_iter': [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "        },\n",
    "        {\n",
    "            'solver': ['saga'],\n",
    "            'penalty': ['elasticnet'],\n",
    "            'C': np.arange(1.0, 10.0, 1.0),\n",
    "            'l1_ratio': [0.1, 0.5, 0.9],\n",
    "            'max_iter': [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "        },\n",
    "        {\n",
    "            'solver': ['saga', 'sag', 'newton-cg', 'lbfgs'],\n",
    "            'penalty': [None],\n",
    "            'max_iter': [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    grid = GridSearchCV(estimator=lr, param_grid=param_grid, verbose=0, n_jobs=-1, cv=2, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters: {grid.best_params_}\")\n",
    "    return grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lr(best_param, X_train, X_test, model_type):\n",
    "    lr = LogisticRegression(solver=best_param['solver'], penalty=best_param['penalty'], C=best_param['C'], max_iter=best_param['max_iter'])\n",
    "    lr.fit(X_train, y_train)\n",
    "    score = evaluate(lr, X_train, X_test, y_train, y_test)\n",
    "    if score > best_score_lr:\n",
    "        best_score_lr = score\n",
    "        best_modeltype_lr = model_type\n",
    "        best_model = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LR_withoutyproc_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_bow_noproc)\n",
    "model_lr(best_params, X_train_bow_noproc, X_test_bow_noproc , name + \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_bow_noproc,y_train)\n",
    "model_lr(best_params, X_train_bow_noproc, X_test_bow_noproc, name + \"bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_ngram_noproc)\n",
    "model_lr(best_params, X_train_ngram_noproc, X_test_ngram_noproc, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_ngram_noproc,y_train)\n",
    "model_lr(best_params, X_train_ngram_noproc, X_test_ngram_noproc, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_tfidf_noproc)\n",
    "model_lr(best_params, X_train_tfidf_noproc, X_test_tfidf_noproc, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_tfidf_noproc,y_train)\n",
    "model_lr(best_params, X_train_tfidf_noproc, X_test_tfidf_noproc, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LR_withproc_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_bow_proc)\n",
    "model_lr(best_params, X_train_bow_proc, X_test_bow_proc, name + \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_bow_proc,y_train)\n",
    "model_lr(best_params, X_train_bow_proc, X_test_bow_proc, name + \"bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_ngram_proc)\n",
    "model_lr(best_params, X_train_ngram_proc, X_test_ngram_proc, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_ngram_proc,y_train)\n",
    "model_lr(best_params, X_train_ngram_proc, X_test_ngram_proc, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_tfidf_proc)\n",
    "model_lr(best_params, X_train_tfidf_proc, X_test_tfidf_proc, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_tfidf_proc,y_train)\n",
    "model_lr(best_params, X_train_tfidf_proc, X_test_tfidf_proc, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With lamatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LR_lem_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_bow_lem)\n",
    "model_lr(best_params, X_train_bow_lem, X_test_bow_lem, name + \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_bow_lem)\n",
    "model_lr(best_params, X_train_bow_lem, X_test_bow_lem, name + \"bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_ngram_lem)\n",
    "model_lr(best_params, X_train_ngram_lem, X_test_ngram_lem, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_bow_lem)\n",
    "model_lr(best_params, X_train_ngram_lem, X_test_ngram_lem, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_tfidf_lem)\n",
    "model_lr(best_params, X_train_tfidf_lem, X_test_tfidf_lem, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_tfidf_lem)\n",
    "model_lr(best_params, X_train_tfidf_lem, X_test_tfidf_lem, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Witth stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LR_stem_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_bow_stem)\n",
    "model_lr(best_params, X_train_bow_stem, X_test_bow_stem, name + \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_bow_stem)\n",
    "model_lr(best_params, X_train_bow_stem, X_test_bow_stem, name + \"bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_ngram_stem)\n",
    "model_lr(best_params, X_train_ngram_stem, X_test_ngram_stem, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_ngram_stem)\n",
    "model_lr(best_params, X_train_ngram_stem, X_test_ngram_stem, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_lr(X_train_tfidf_stem)\n",
    "model_lr(best_params, X_train_tfidf_stem, X_test_tfidf_stem, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_tfidf_stem)\n",
    "model_lr(best_params, X_train_tfidf_stem, X_test_tfidf_stem, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_model,f'Models/LR/{best_modeltype_lr}.pkl')\n",
    "print(best_score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "**Important Parameters:**\n",
    "- Var Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_nb = 0\n",
    "best_modeltype_nb = \"\"\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_nb(X_train):\n",
    "    space = {\n",
    "        'alpha': hp.uniform('alpha', 0.0, 2.0),\n",
    "        'fit_prior': hp.choice('fit_prior', [True, False]),\n",
    "        'class_prior': hp.choice('class_prior', [None, hp.uniform('class_prior_uniform',0.0,1.0)])\n",
    "    }\n",
    "\n",
    "    def objective(params):\n",
    "        clf = MultinomialNB(alpha=params['alpha'], fit_prior=params['fit_prior'], class_prior=params['class_prior'])\n",
    "        score = cross_val_score(clf, X_train.toarray(), y_train, cv=5).mean()\n",
    "        return -score\n",
    "\n",
    "    tpe_algorithm = tpe.suggest\n",
    "\n",
    "    trials = Trials()\n",
    "\n",
    "    best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe_algorithm,\n",
    "            max_evals=50,\n",
    "            trials=trials)\n",
    "\n",
    "    best_params = space_eval(space, best)\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_nb(X_train):\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    param_grid = {\n",
    "        'alpha': [0.0, 0.1, 0.5, 1.0, 2.0],\n",
    "        'fit_prior': [True, False],\n",
    "        'class_prior': [None, [0.1, 0.9],[0.2, 0.8],[0.3, 0.7],[0.4, 0.6]]\n",
    "    }\n",
    "\n",
    "    gs_nb = GridSearchCV(estimator=nb, param_grid=param_grid, verbose=0, n_jobs=-1, cv=2, scoring='accuracy')\n",
    "    gs_nb.fit(X_train.toarray(), y_train)\n",
    "\n",
    "    print(f\"Best parameters: {gs_nb.best_params_}\")\n",
    "    return gs_nb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_nb(best_param, X_train, X_test, model_type):\n",
    "    nb = MultinomialNB(var_smoothing=best_param['var_smoothing'])\n",
    "    nb.fit(X_train, y_train)\n",
    "    score = evaluate(nb, X_train, X_test, y_train, y_test)\n",
    "    if score > best_score_nb:\n",
    "        best_score_nb = score\n",
    "        best_modeltype_nb = model_type\n",
    "        best_model = nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"NB_withoutyproc_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_bow_noproc)\n",
    "model_nb(best_params, X_train_bow_noproc, X_test_bow_noproc , name + \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_bow_noproc,y_train)\n",
    "model_nb(best_params, X_train_bow_noproc, X_test_bow_noproc, name + \"bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_ngram_noproc)\n",
    "model_nb(best_params, X_train_ngram_noproc, X_test_ngram_noproc, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_ngram_noproc,y_train)\n",
    "model_nb(best_params, X_train_ngram_noproc, X_test_ngram_noproc, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_tfidf_noproc)\n",
    "model_nb(best_params, X_train_tfidf_noproc, X_test_tfidf_noproc, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_tfidf_noproc,y_train)\n",
    "model_nb(best_params, X_train_tfidf_noproc, X_test_tfidf_noproc, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"NB_withproc_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_bow_proc)\n",
    "model_nb(best_params, X_train_bow_proc, X_test_bow_proc, name + \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_bow_proc,y_train)\n",
    "model_nb(best_params, X_train_bow_proc, X_test_bow_proc, name + \"bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_ngram_proc)\n",
    "model_nb(best_params, X_train_ngram_proc, X_test_ngram_proc, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_ngram_proc,y_train)\n",
    "model_nb(best_params, X_train_ngram_proc, X_test_ngram_proc, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_tfidf_proc)\n",
    "model_nb(best_params, X_train_tfidf_proc, X_test_tfidf_proc, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_tfidf_proc,y_train)\n",
    "model_nb(best_params, X_train_tfidf_proc, X_test_tfidf_proc, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With lamatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"NB_lem_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_bow_lem)\n",
    "model_nb(best_params, X_train_bow_lem, X_test_bow_lem, name + \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_bow_lem)\n",
    "model_nb(best_params, X_train_bow_lem, X_test_bow_lem, name + \"bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_ngram_lem)\n",
    "model_nb(best_params, X_train_ngram_lem, X_test_ngram_lem, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_bow_lem)\n",
    "model_nb(best_params, X_train_ngram_lem, X_test_ngram_lem, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_tfidf_lem)\n",
    "model_nb(best_params, X_train_tfidf_lem, X_test_tfidf_lem, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_tfidf_lem)\n",
    "model_nb(best_params, X_train_tfidf_lem, X_test_tfidf_lem, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Witth stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"NB_stem_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_bow_stem)\n",
    "model_nb(best_params, X_train_bow_stem, X_test_bow_stem, name + \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_lr(X_train_bow_stem)\n",
    "model_nb(best_params, X_train_bow_stem, X_test_bow_stem, name + \"bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_ngram_stem)\n",
    "model_nb(best_params, X_train_ngram_stem, X_test_ngram_stem, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_ngram_stem)\n",
    "model_nb(best_params, X_train_ngram_stem, X_test_ngram_stem, name + \"ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperopt_nb(X_train_tfidf_stem)\n",
    "model_nb(best_params, X_train_tfidf_stem, X_test_tfidf_stem, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_nb(X_train_tfidf_stem)\n",
    "model_nb(best_params, X_train_tfidf_stem, X_test_tfidf_stem, name + \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_model,f'Models/NB/{best_modeltype_nb}.pkl')\n",
    "print(best_score_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machinne\n",
    "\n",
    "**Important Parameters:**\n",
    "- Kernel\n",
    "- Gamma\n",
    "- C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_svm(X_train):\n",
    "    space = {\n",
    "        'C': hp.choice('C', [0.1, 1, 10, 100, 1000]),\n",
    "        'gamma': hp.choice('gamma', [1, 0.1, 0.01, 0.001, 0.0001]),\n",
    "        'kernel': hp.choice('kernel', ['rbf', 'linear'])\n",
    "    }\n",
    "\n",
    "    def objective(params):\n",
    "        clf = SVC(C=1, gamma=params['gamma'], kernel=params['kernel'])\n",
    "        score = cross_val_score(clf, X_train.toarray(), y_train, cv=5).mean()\n",
    "        return -score \n",
    "\n",
    "    tpe_algorithm = tpe.suggest\n",
    "\n",
    "    trials = Trials()\n",
    "\n",
    "    best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe_algorithm,\n",
    "            max_evals=50,\n",
    "            trials=trials)\n",
    "\n",
    "    best_params = space_eval(space, best)\n",
    "    print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_svm(X_train):\n",
    "    svm = SVC()\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "\n",
    "    gs_svm = GridSearchCV(estimator=svm, param_grid=param_grid, verbose=0, n_jobs=-1, cv=2, scoring='accuracy')\n",
    "    gs_svm.fit(X_train.toarray(), y_train)\n",
    "\n",
    "    print(f\"Best parameters: {gs_svm.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function Support Vector Machine ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=0.1, gamma=1,kernel='rbf', probability=True)\n",
    "model = svm.fit(X_train, y_train)\n",
    "\n",
    "evaluate(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN model for text classification\n",
    "vocab_size = 5000\n",
    "embedding_dim = 32\n",
    "\n",
    "rnn = Sequential(name=\"Basic_RNN\")\n",
    "rnn.add(Embedding(vocab_size, embedding_dim))\n",
    "rnn.add(SimpleRNN(128, activation='relu', return_sequences= True))\n",
    "rnn.add(SimpleRNN(64, activation='relu', return_sequences= True))\n",
    "rnn.add(SimpleRNN(32, activation='relu'))\n",
    "rnn.add(Dense(4, activation='softmax'))\n",
    "\n",
    "rnn.build(input_shape=(None, X_train.shape[1]))\n",
    "\n",
    "print(rnn.summary())\n",
    "\n",
    "rnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#history = rnn.fit(X_train.toarray(), y_train, epochs=10, validation_data=(X_test.toarray(), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Conv1D model for text classification\n",
    "conv1d = Sequential(name=\"Conv1D\")\n",
    "conv1d.add(Embedding(vocab_size, embedding_dim))\n",
    "conv1d.add(Conv1D(128, 5, activation='relu'))\n",
    "conv1d.add(GlobalMaxPooling1D())\n",
    "conv1d.add(Dense(64, activation='relu'))\n",
    "conv1d.add(Dense(4, activation='softmax'))\n",
    "\n",
    "conv1d.build(input_shape=(None, X_train.shape[1]))\n",
    "\n",
    "print(conv1d.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model for text classification\n",
    "lstm = Sequential(name=\"LSTM\")\n",
    "lstm.add(Embedding(vocab_size, embedding_dim))\n",
    "lstm.add(LSTM(32))\n",
    "lstm.add(Dropout(0.4))\n",
    "lstm.add(Dense(4, activation='softmax'))\n",
    "\n",
    "lstm.build(input_shape=(None, X_train.shape[1]))\n",
    "\n",
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model for text classification\n",
    "blstm = Sequential(name=\"Bi_LSTM\")\n",
    "blstm.add(Embedding(vocab_size, embedding_dim))\n",
    "blstm.add(Bidirectional(LSTM(64, return_sequences=True, input_shape=(None, 1))))\n",
    "blstm.add(Dropout(0.4))\n",
    "blstm.add(Bidirectional(LSTM(32)))\n",
    "blstm.add(Dropout(0.2))\n",
    "blstm.add(Dense(64, activation='relu'))\n",
    "blstm.add(Dropout(0.1))\n",
    "blstm.add(Dense(4, activation='softmax'))\n",
    "\n",
    "blstm.build(input_shape=(None, X_train.shape[1]))\n",
    "\n",
    "print(blstm.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
